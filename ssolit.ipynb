{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local installs\n",
    "# !pip install dill\n",
    "# !pip install xgboost\n",
    "# !pip install sklearn==0.0.post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive = False;\n",
    "path_head = \"\"\n",
    "\n",
    "if(gdrive):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  path_head = '/content/drive/MyDrive/Cis523/Bounty/'\n",
    "  !pip install dill\n",
    "  !pip install xgboost\n",
    "  !pip install scikit-learn==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import xgboost as xg\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "1.7.5\n"
     ]
    }
   ],
   "source": [
    "print(sk.__version__)\n",
    "print(xg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and make subsets \n",
    "x_train = pd.read_csv(path_head + 'training_data.csv') \n",
    "y_train = np.genfromtxt(path_head + 'training_labels.csv', delimiter=',', dtype = float).reshape(340134,1)\n",
    "\n",
    "x_train_subset, x_val, y_train_subset, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size = .15, random_state = 42)\n",
    "# PDL = PointerDecisionList(base_clf, x_train_subset, y_train_subset, x_val, y_val, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import current info\n",
    "# Note, may not be up to date with global git\n",
    "global_preds_path = path_head + 'models/global_model/training_predictions.csv'\n",
    "global_preds = pd.read_csv(global_preds_path, header=None).transpose()\n",
    "PAS_preds_path = path_head + 'models/PAS/training_predictions.csv'\n",
    "PAS_preds = pd.read_csv(PAS_preds_path, header=None).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, old_train_preds, old_val_preds = sk.model_selection.train_test_split(x_train, global_preds, test_size = .15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check improvement on validation data\n",
    "def check_improvement(old_pred, g, h):\n",
    "    indices = g(x_val)\n",
    "    new_pred = h(x_val[indices])\n",
    "    \n",
    "    improvement = -100000\n",
    "    if(len(new_pred)!=0):   \n",
    "        old_RMSE = math.sqrt(mean_squared_error(y_val[indices], old_val_preds[indices]))\n",
    "        new_RMSE = math.sqrt(mean_squared_error(y_val[indices], new_pred))\n",
    "        improvement = old_RMSE-new_RMSE\n",
    "    print(f\"improvement: {improvement}\")\n",
    "    if (improvement>0):\n",
    "        print(\"\\n IMPROVEMENT \\n IMPROVEMENT \\n IMPROVEMENT \\n\")\n",
    "    return improvement\n",
    "\n",
    "def check_local_improvement(g,h):\n",
    "    return check_improvement(PAS_preds, g, h)\n",
    "\n",
    "def check_global_improvement(g,h):\n",
    "    return check_improvement(global_preds, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_improvement(old_pred, g, h):\n",
    "#     indices = g(x_val)\n",
    "#     old_pred = old_pred[indices]\n",
    "#     new_pred = h(x_train[indices])\n",
    "#     old_RMSE = math.sqrt(mean_squared_error(y_train[indices], old_pred))\n",
    "#     new_RMSE = math.sqrt(mean_squared_error(y_train[indices], new_pred))\n",
    "#     # print(f\"improvement: {old_RMSE-new_RMSE}\")\n",
    "#     if (old_RMSE-new_RMSE>0):\n",
    "#         print(\"\\n IMPROVEMENT \\n IMPROVEMENT \\n IMPROVEMENT \\n\")\n",
    "#     return old_RMSE-new_RMSE\n",
    "\n",
    "# def check_local_improvement(g,h):\n",
    "#     return check_improvement(PAS_preds, g, h)\n",
    "\n",
    "# def check_global_improvement(g,h):\n",
    "#     return check_improvement(global_preds, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkls(g,h):\n",
    "    with open(path_head + 'g.pkl', 'wb') as file:\n",
    "        pkl.dump(g, file)\n",
    "\n",
    "    # save hypothesis function to h.pkl\n",
    "    with open(path_head + 'h.pkl', 'wb') as file:\n",
    "        pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic_h(g):\n",
    "    clf = sk.tree.DecisionTreeRegressor(max_depth = 7, random_state = 42)\n",
    "\n",
    "    # find group indices on data\n",
    "    indices = g(x_train_subset)\n",
    "\n",
    "    # fit model specifically to group\n",
    "    clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "    # define hypothesis function as bound clf.predict\n",
    "    h = clf.predict\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_XGBRegressor(g):\n",
    "    clf = XGBRegressor(max_depth = 10, random_state = 42)\n",
    "    indices = g(x_train_subset)\n",
    "    clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "    h = clf.predict\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help minimize start up difficulties, we have provided you with a basic ML workflow for this project, as well as a few possible avenues to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 1: ML Workflow for Submitting *(g,h)* pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Pip Installs and Imports\n",
    "\n",
    "We will be using a package *dill* which is a variant of *pickle*, but allows a bit more expressive byte code serialization. This package is essential to saving your *(g,h)* pairs!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a non-inclusive list of packages you may find helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download/Load Data\n",
    "\n",
    "Navigate to the project [webpage](https://declancharrison.github.io/CIS_5230_Bias_Bounty_2023/) and click \"Download Training Data\". Extract the .zip files in the folder where this notebook is located, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('training_data.csv') \n",
    "y_train = np.genfromtxt('training_labels.csv', delimiter=',', dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a (g,h) pair\n",
    "\n",
    "Below is an example of training a Decision Tree Regressor on individuals identified as white from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 1\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 5, random_state = 42)\n",
    "\n",
    "# find group indices on data\n",
    "indices = g(x_train)\n",
    "\n",
    "# fit model specifically to group\n",
    "clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Save Objects\n",
    "\n",
    "The following cell will save your group model *g* with filename *g.pkl*, and your hypothesis function *h* with filename *h.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group function to g.pkl\n",
    "with open('g.pkl', 'wb') as file:\n",
    "    pkl.dump(g, file)\n",
    "\n",
    "# save hypothesis function to h.pkl\n",
    "with open('h.pkl', 'wb') as file:\n",
    "    pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Upload Models to Google Drive and Submit PR Request with Links\n",
    "\n",
    "Follow instructions on GitHub Repo to submit a *(g,h)* pair update request!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: Reducing Workflow Time Requirements by Creating a Local PDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probably noticed, submitting a *(g,h)* pair to the GitHub repository can take a long time depending on the current workload of the server. To approximate whether or not an update will be accepted, we have provided you the PDL architecture file and a workflow that will mimic your team's private PDL maintained by the server. \n",
    "\n",
    "**NOTE: One major caveat is the validation data this workflow uses is a cut from the training data, meaning you will want to refrain from training on it to prevent overfitting.**\n",
    "\n",
    "The way we suggest getting around this without losing data efficacy is to train a *(g,h)* pair on the subset of training data that does not include the validation set, and attempt the *(g,h)* pair update on the local PDL. If the pair is rejected, you can continue tuning hyperparameters or searching for new groups. If the pair is accepted, you can retrain a new *(g,h)* pair over ALL the training data, and submit this pair to the server for an update. This will allow you to \"squeeze all the juice\" from your training data and test potential updates much quicker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT CHANGE THIS CELL ###\n",
    "from pdl import PointerDecisionList\n",
    "\n",
    "x_train_subset, x_val, y_train_subset, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size = .15, random_state = 42)\n",
    "base_clf = sk.tree.DecisionTreeRegressor(max_depth = 1, random_state = 42)\n",
    "base_clf.fit(x_train_subset, y_train_subset)\n",
    "PDL = PointerDecisionList(base_clf, x_train_subset, y_train_subset, x_val, y_val, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your *(g,h)* pair on the subset of training data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 2\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 5, random_state = 42)\n",
    "\n",
    "# find group indices on data\n",
    "indices = g(x_train_subset)\n",
    "\n",
    "# fit model specifically to group\n",
    "clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt an update using the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_flag = PDL.update(g, h, x_train_subset, y_train_subset, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put these two together to train a classifier using the whole training dataset after if it has been accepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 1\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 10, random_state = 42)\n",
    "\n",
    "# find group indices on training subset\n",
    "indices = g(x_train_subset)\n",
    "\n",
    "# fit model specifically to group subset\n",
    "clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict\n",
    "\n",
    "# compute PDL update\n",
    "update_flag = PDL.update(g, h, x_train_subset, y_train_subset, x_val, y_val)\n",
    "\n",
    "if update_flag:\n",
    "\n",
    "    # recompute indices over whole training dataset\n",
    "    indices = g(x_train)\n",
    "\n",
    "    # refit classifier to full group\n",
    "    clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "    # define hypothesis function as bound clf.predict\n",
    "    h = clf.predict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit *(g,h)* pair to GitHub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: You can save your PDL but it will require that your validation set does not change! Thus, you should not change the random state used to split your training data once you create your PDL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save PDL\n",
    "PDL.save_model()\n",
    "\n",
    "# open PDL structure\n",
    "with open('PDL/model.pkl', 'rb') as file:\n",
    "    PDL = pkl.load(file)\n",
    "\n",
    "# reload group/hypothesis functions to PDL\n",
    "PDL.reload_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Ahmed's groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supgroups from Original Bias Bounties Paper\n",
    "\n",
    "# male subgroup\n",
    "def g_o1(X):\n",
    "    return X['SEX'] == 1\n",
    "\n",
    "# female subgroup\n",
    "def g_o2(X):\n",
    "    return X['SEX'] == 2\n",
    "\n",
    "# ages 17-24 and non-white\n",
    "def g_o3(X):\n",
    "  return (X['RAC1P'] != 1.0) & (X['AGEP'] >= 17) & (X['AGEP'] <= 24)\n",
    "\n",
    "# over the age of 30 working without pay in a family business\n",
    "def g_o4(X):\n",
    "  return (X['AGEP'] > 30) & (X['COW'] == 8.0)\n",
    "\n",
    "# self-employed in own not incorporated business, professional practice, or farm\n",
    "def g_o5(X):\n",
    "  return X['COW'] == 6.0\n",
    "\n",
    "# over the age of 62 (retired)\n",
    "def g_o6(X):\n",
    "  return (X['AGEP'] > 62) & (X['COW'] == 9.0)\n",
    "\n",
    "# First-line supervisors of office and administrative support workers\n",
    "def g_o7(X):\n",
    "  return X['OCCP'] == 5000\n",
    "\n",
    "# real estate brokers and sales agents\n",
    "def g_o8(X):\n",
    "  return X['OCCP'] == 4920\n",
    "\n",
    "# real estate brokers and sales agents\n",
    "def g_o9(X):\n",
    "  return X['OCCP'] == 4700\n",
    "\n",
    "# office clerks, general\n",
    "def g_o10(X):\n",
    "  return X['OCCP'] == 5860\n",
    "\n",
    "#paint, coating, and adhesive manufacturing\n",
    "def g_o11(X):\n",
    "  return X['OCCP'] == 2270\n",
    "\n",
    "# those born in California, Mexico, or Southeast Asia generally working as medical technicians\n",
    "def g_o12(X):\n",
    "  return (X['OCCP'] == 3401) | (X['ST'] == 6.0)\n",
    "\n",
    "# accountants/Auditors who work 40 hour work weeks\n",
    "def g_o13(X):\n",
    "  return (X['OCCP'] == 800) & (X['WKHP'] >= 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroups that seem to be interesting\n",
    "\n",
    "# military involvement\n",
    "def g_m1(X):\n",
    "  return X['MIL'] != 4.0\n",
    "\n",
    "# minimal education\n",
    "def g_m2(X):\n",
    "  return X['SCHL'] < 16.0\n",
    "\n",
    "# high education\n",
    "def g_m3(X):\n",
    "  return X['SCHL'] > 21.0 \n",
    "\n",
    "# very little hours worked per week\n",
    "def g_m4(X):\n",
    "  return X['WKHP'] < 10 \n",
    "\n",
    "# less than normal hours worked per week\n",
    "def g_m5(X):\n",
    "  return (X['WKHP'] >= 10) & (X['WKHP'] < 30)\n",
    "\n",
    "# normal hours worked per week\n",
    "def g_m6(X):\n",
    "  return (X['WKHP'] >= 30) & (X['WKHP'] < 50)\n",
    "\n",
    "# high hours worked per week\n",
    "def g_m7(X):\n",
    "  return (X['WKHP'] >= 50) & (X['WKHP'] < 80)\n",
    "\n",
    "# very high hours worked per week\n",
    "def g_m8(X):\n",
    "  return (X['WKHP'] >= 80)\n",
    "\n",
    "# people with disabilities\n",
    "def g_m9(X):\n",
    "  return (X['DDRS'] == 1.0) | (X['DEAR'] == 1.0) | (X['DEYE'] == 1.0) | \\\n",
    "  (X['DOUT'] == 1.0) | (X['DRAT'] == 1.0) | (X['DREM'] == 1.0)\n",
    "\n",
    "# non-married men \n",
    "def g_m10(X):\n",
    "  return (X['MAR'] != 1.0) & (X['SEX'] == 1.0)\n",
    "\n",
    "# non-married women\n",
    "def g_m11(X):\n",
    "  return (X['MAR'] != 1.0) & (X['SEX'] == 2.0)\n",
    "\n",
    "# software developers\n",
    "def g_m12(X):\n",
    "  return X['OCCP'] == 1021.0 \n",
    "\n",
    "# black females \n",
    "def g_m13(X):\n",
    "  return (X['SEX'] == 2.0) & (X['RAC1P'] == 2.0)\n",
    "\n",
    "# asian men\n",
    "def g_m14(X):\n",
    "  return (X['SEX'] == 1.0) & (X['RAC1P'] == 6.0)\n",
    "\n",
    "# people who take public transporation / walk / bike\n",
    "def g_m15(X):\n",
    "  return (X['JWTRNS'] == 2.0) | (X['JWTRNS'] == 3.0) | (X['JWTRNS'] == 5.0) | \\\n",
    "           (X['JWTRNS'] == 10) | (X['JWTRNS'] == 9.0)\n",
    "\n",
    "# non white/asian recent mothers\n",
    "def g_m16(X):\n",
    "  return (X['FER'] == 1.0) & (X['RAC1P'] != 1.0) & (X['RAC1P'] != 6.0)\n",
    "\n",
    "# teenagers\n",
    "def g_m18(X):\n",
    "  return (X['AGEP'] > 12.0) & (X['AGEP'] <= 19.0)\n",
    "\n",
    "# young adults\n",
    "def g_m19(X):\n",
    "  return (X['AGEP'] > 19) & (X['AGEP'] <= 27.0)\n",
    "\n",
    "# adults\n",
    "def g_m20(X):\n",
    "  return (X['AGEP'] > 27) & (X['AGEP'] <= 55.0)\n",
    "\n",
    "# old ppl\n",
    "def g_m21(X):\n",
    "  return (X['AGEP'] > 55.0)\n",
    "\n",
    "# managers \n",
    "def g_m22(X):\n",
    "  return X['OCCP'] <= 440.0\n",
    "\n",
    "# engineers\n",
    "def g_m23(X):\n",
    "  return (X['OCCP'] >= 1305.0) & (X['OCCP'] <= 1555)\n",
    "\n",
    "# teachers\n",
    "def g_m24(X):\n",
    "  return (X['OCCP'] >= 2205.0) & (X['OCCP'] <= 2555.0)\n",
    "\n",
    "# people in medicine\n",
    "def g_m25(X):\n",
    "  return (X['OCCP'] >= 3000.0) & (X['OCCP'] <= 3550.0)\n",
    "\n",
    "# speaks more than english\n",
    "def g_m27(X):\n",
    "  return X['LANX'] == 1.0 \n",
    "\n",
    "# speaks only english\n",
    "def g_m28(X):\n",
    "  return X['LANX'] == 2.0\n",
    "\n",
    "# self employed inc black male \n",
    "def g_m29(X):\n",
    "  return (X['COW'] == 7.0) & (X['RAC1P'] == 2.0) & (X['SEX'] == 1.0)\n",
    "\n",
    "# self employed inc black female \n",
    "def g_m30(X):\n",
    "  return (X['COW'] == 7.0) & (X['RAC1P'] == 2.0) & (X['SEX'] == 2.0)\n",
    "\n",
    "# self employed inc asian male \n",
    "def g_m31(X):\n",
    "  return (X['COW'] == 7.0) & (X['RAC1P'] == 6.0) & (X['SEX'] == 1.0)\n",
    "\n",
    "# self employed inc asian female \n",
    "def g_m32(X):\n",
    "  return (X['COW'] == 7.0) & (X['RAC1P'] == 6.0) & (X['SEX'] == 2.0)\n",
    "\n",
    "# over 75 and self employed\n",
    "def g_m33(X):\n",
    "  return (X['COW'] == 7.0) & (X['AGEP'] >= 75)\n",
    "\n",
    "# self employed not inc black male \n",
    "def g_m34(X):\n",
    "  return (X['COW'] == 6.0) & (X['RAC1P'] == 2.0) & (X['SEX'] == 1.0)\n",
    "\n",
    "# self employed not inc black female \n",
    "def g_m35(X):\n",
    "  return (X['COW'] == 6.0) & (X['RAC1P'] == 2.0) & (X['SEX'] == 2.0)\n",
    "\n",
    "# self employed not inc asian male \n",
    "def g_m36(X):\n",
    "  return (X['COW'] == 6.0) & (X['RAC1P'] == 6.0) & (X['SEX'] == 1.0)\n",
    "\n",
    "# self employed not inc asian female \n",
    "def g_m37(X):\n",
    "  return (X['COW'] == 6.0) & (X['RAC1P'] == 6.0) & (X['SEX'] == 2.0)\n",
    "\n",
    "# over 75 and self employed\n",
    "def g_m38(X):\n",
    "  return (X['COW'] == 6.0) & (X['AGEP'] >= 75)\n",
    "\n",
    "# people who live in NY / CALI\n",
    "def g_m39(X):\n",
    "  return (X['ST'] == 6.0) | (X['ST'] == 36.0)\n",
    "\n",
    "# some college\n",
    "def g_m40(X):\n",
    "  return (X['SCHL'] > 16.0) & (X['SCHL'] < 21.0)\n",
    "\n",
    "# bachelor degree\n",
    "def g_m41(X):\n",
    "  return X['SCHL'] == 21.0\n",
    "\n",
    "# self employed and works < 10\n",
    "def g_m42(X):\n",
    "  return ((X['COW'] == 6.0) | (X['COW'] == 7.0)) & (X['WKHP'] <= 10)\n",
    "\n",
    "# self employed and works [11, 20] hours\n",
    "def g_m43(X):\n",
    "  return ((X['COW'] == 6.0) | (X['COW'] == 7.0)) & ((X['WKHP'] > 10) & (X['WKHP'] <= 20))\n",
    "\n",
    "# self employed and works [31, 60] hours\n",
    "def g_m44(X):\n",
    "  return ((X['COW'] == 6.0) | (X['COW'] == 7.0)) & ((X['WKHP'] > 30) & (X['WKHP'] <= 60))\n",
    "\n",
    "# self employed and works > 60 hours\n",
    "def g_m45(X):\n",
    "  return ((X['COW'] == 6.0) | (X['COW'] == 7.0)) & (X['WKHP'] > 60)\n",
    "\n",
    "# private profit and works < 10\n",
    "def g_m46(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] <= 10)\n",
    "\n",
    "# self employed and works [11, 20] hours\n",
    "def g_m47(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 10) & (X['WKHP'] <= 20)\n",
    "\n",
    "# self employed and works [31, 60] hours\n",
    "def g_m48(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 30) & (X['WKHP'] <= 60)\n",
    "\n",
    "# self employed and works > 60 hours\n",
    "def g_m49(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 60)\n",
    "\n",
    "# private non-profit and works < 10\n",
    "def g_m50(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] <= 10)\n",
    "\n",
    "# private non-profit and works [11, 20] hours\n",
    "def g_m51(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 10) & (X['WKHP'] <= 20)\n",
    "\n",
    "# private non-profit and works [31, 60] hours\n",
    "def g_m52(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 30) & (X['WKHP'] <= 60)\n",
    "\n",
    "# private non-profit and works > 60 hours\n",
    "def g_m53(X):\n",
    "  return (X['COW'] == 1.0) & (X['WKHP'] > 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager\n",
    "def g_m54(X):\n",
    "  return X['OCCP'] <= 440\n",
    "\n",
    "# business\n",
    "def g_m55(X):\n",
    "  return (X['OCCP'] >= 500) & (X['OCCP'] <= 750)\n",
    "\n",
    "# finance\n",
    "def g_m56(X):\n",
    "  return (X['OCCP'] >= 800) & (X['OCCP'] <= 960)\n",
    "\n",
    "# communication\n",
    "def g_m57(X):\n",
    "  return (X['OCCP'] >= 1005) & (X['OCCP'] <= 1240)\n",
    "\n",
    "# engineering\n",
    "def g_m58(X):\n",
    "  return (X['OCCP'] >= 1305) & (X['OCCP'] <= 1560)\n",
    "\n",
    "# science\n",
    "def g_m59(X):\n",
    "  return (X['OCCP'] >= 1600) & (X['OCCP'] <= 1980)\n",
    "\n",
    "# cms\n",
    "def g_m60(X):\n",
    "  return (X['OCCP'] >= 2001) & (X['OCCP'] <= 2060)\n",
    "\n",
    "# legal\n",
    "def g_m61(X):\n",
    "  return (X['OCCP'] >= 2105) & (X['OCCP'] <= 2180)\n",
    "\n",
    "# education\n",
    "def g_m62(X):\n",
    "  return (X['OCCP'] >= 2205) & (X['OCCP'] <= 2555)\n",
    "\n",
    "# ENT\n",
    "def g_m63(X):\n",
    "  return (X['OCCP'] >= 2600) & (X['OCCP'] <= 2920)\n",
    "\n",
    "# medicine\n",
    "def g_m63(X):\n",
    "  return (X['OCCP'] >= 3000) & (X['OCCP'] <= 3550)\n",
    "\n",
    "# hls\n",
    "def g_m63(X):\n",
    "  return (X['OCCP'] >= 3601) & (X['OCCP'] <= 3655)\n",
    "\n",
    "# prt\n",
    "def g_m64(X):\n",
    "  return (X['OCCP'] >= 3700) & (X['OCCP'] <= 3960)\n",
    "\n",
    "# cln\n",
    "def g_m65(X):\n",
    "  return (X['OCCP'] >= 4000) & (X['OCCP'] <= 4160)\n",
    "\n",
    "# prs\n",
    "def g_m66(X):\n",
    "  return (X['OCCP'] >= 4200) & (X['OCCP'] <= 4255)\n",
    "\n",
    "# sal\n",
    "def g_m67(X):\n",
    "  return (X['OCCP'] >= 4330) & (X['OCCP'] <= 4655)\n",
    "\n",
    "# off\n",
    "def g_m68(X):\n",
    "  return (X['OCCP'] >= 4700) & (X['OCCP'] <= 4965)\n",
    "\n",
    "# fff\n",
    "def g_m69(X):\n",
    "  return (X['OCCP'] >= 5000) & (X['OCCP'] <= 5940)\n",
    "\n",
    "# con\n",
    "def g_m70(X):\n",
    "  return (X['OCCP'] >= 6005) & (X['OCCP'] <= 6130)\n",
    "\n",
    "# ext\n",
    "def g_m71(X):\n",
    "  return (X['OCCP'] >= 6200) & (X['OCCP'] <= 6765)\n",
    "\n",
    "# rpr\n",
    "def g_m72(X):\n",
    "  return (X['OCCP'] >= 6800) & (X['OCCP'] <= 6950)\n",
    "\n",
    "# prd\n",
    "def g_m73(X):\n",
    "  return (X['OCCP'] >= 7000) & (X['OCCP'] <= 7640)\n",
    "\n",
    "# trn \n",
    "def g_m74(X):\n",
    "  return (X['OCCP'] >= 7700) & (X['OCCP'] <= 8990)\n",
    "\n",
    "# military\n",
    "def g_m75(X):\n",
    "  return (X['OCCP'] >= 9005) & (X['OCCP'] <= 9760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups_original = [g_o1, g_o2, g_o3, g_o4, g_o5, g_o6, g_o7, g_o8, g_o9, g_o10,\n",
    "                      g_o11, g_o12, g_o13]\n",
    "subgroups_manual = [g_m1, g_m2, g_m3, g_m4, g_m5, g_m6, g_m7, g_m8, g_m9, g_m10,\n",
    "                    g_m11, g_m12, g_m13, g_m14, g_m15, g_m16, g_m18, g_m19,\n",
    "                    g_m20, g_m21, g_m22, g_m23, g_m24, g_m25, g_m27, g_m28,\n",
    "                    g_m29, g_m30, g_m31, g_m32, g_m33, g_m34, g_m35, g_m36,\n",
    "                    g_m37, g_m38, g_m39, g_m40, g_m41, g_m42, g_m43, g_m44,\n",
    "                    g_m45, g_m46, g_m47, g_m48, g_m49, g_m50, g_m51, g_m52,\n",
    "                    g_m53, g_m54, g_m55, g_m56, g_m57, g_m58, g_m59, g_m60,\n",
    "                    g_m61, g_m62, g_m63, g_m64, g_m65, g_m66, g_m67, g_m68,\n",
    "                    g_m69, g_m70, g_m71, g_m72, g_m73, g_m74, g_m75]\n",
    "\n",
    "manual_groups = subgroups_original + subgroups_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automated Group Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Epsilon above/below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view how different global is from labels\n",
    "abs_diff = (global_preds - y_train).abs()\n",
    "abs_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train clf to identify rows with big difference\n",
    "\n",
    "def epsilon_above(epsilon):\n",
    "    # define 0,1 labels where current predictions OVERESTIMATE by at least epsilon\n",
    "    binary_labels = (global_preds - y_train) < epsilon\n",
    "\n",
    "    # define group classifier class\n",
    "    clf = sk.tree.DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "\n",
    "    # fit classifier to binary labels\n",
    "    clf.fit(x_train, binary_labels)\n",
    "\n",
    "    # define g\n",
    "    g = clf.predict\n",
    "    # visualize results\n",
    "    # pd.DataFrame(g(x_train).astype(int)).describe()\n",
    "    \n",
    "    return g\n",
    "\n",
    "def epsilon_below(epsilon):\n",
    "    # define 0,1 labels where current predictions OVERESTIMATE by at least epsilon\n",
    "    binary_labels = (y_train - global_preds) < epsilon\n",
    "\n",
    "    # define group classifier class\n",
    "    clf = sk.tree.DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "\n",
    "    # fit classifier to binary labels\n",
    "    clf.fit(x_train, binary_labels)\n",
    "\n",
    "    # define g\n",
    "    g = clf.predict\n",
    "    # visualize results\n",
    "    # pd.DataFrame(g(x_train).astype(int)).describe()\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    g = epsilon_below(i*5000)\n",
    "    train_XGBRegressor(g)\n",
    "    print(str(i*1000))\n",
    "    print(check_global_improvement(g,h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Targeted Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class targeted_correction:\n",
    "    def __init__(self, clf, value, epsilon):\n",
    "        self.clf = clf\n",
    "        self.value = value\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = self.clf.predict(X)\n",
    "        return abs(predictions - self.value) < self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class XGBRegressor_wrap:\n",
    "#     def __init__(self, clf):\n",
    "#         self.clf = clf\n",
    "#     def __call__(self, X):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=7, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=7, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=7, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 7, random_state = 42)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "for i in range(100):\n",
    "    g = targeted_correction(clf, i*1000, 5000)\n",
    "    indices = g(x_train)\n",
    "    # print(indices)\n",
    "    if (indices.any()):\n",
    "        old_RMSE = math.sqrt(mean_squared_error(y_train[indices], global_preds[indices]))\n",
    "        print(i, old_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100):\n",
    "    g = targeted_correction(clf, i*1000, 10000)\n",
    "    h = train_XGBRegressor(g)\n",
    "    if(check_global_improvement(g,h)>0):\n",
    "        print(i, \"found improvement\")\n",
    "        save_pkls(g,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1):\n",
    "    # print(\"i = {}\".format(i))\n",
    "    for j in range(5, 35):\n",
    "        v = i*1000\n",
    "        e = j*400\n",
    "        print(\"v = {}, e = {}\".format(v, e))\n",
    "        g = targeted_correction(clf, e, j)\n",
    "        # h = train_XGBRegressor(g)\n",
    "        if (check_global_improvement(g,h) > 0):\n",
    "                print(v,e)\n",
    "                break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cluster_n:\n",
    "\n",
    "    def __init__(self, clf, n):\n",
    "        # define attibutes here. You may add more parameters to the init method (see example below)\n",
    "        self.clf = clf  \n",
    "        self.n = n\n",
    "\n",
    "    # DO NOT CHANGE CALL FUNCTION, FORMAT .predict\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # find instances where cluster is 1\n",
    "        x_copy = X\n",
    "        X_scaled = scaler.transform(x_copy)\n",
    "        return self.clf.predict(X_scaled) == self.n\n",
    "\n",
    "    \n",
    "cluster_clf = sk.cluster.KMeans(n_clusters= 5, random_state = 42, n_init=10)\n",
    "X_scaled = scaler.transform(x_train_subset)\n",
    "cluster_clf.fit(X_scaled)\n",
    "g = cluster_n(cluster_clf, 1)\n",
    "\n",
    "# visualize results\n",
    "g(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_pkls(g, h, i=\"\"):\n",
    "    # save group function to g.pkl\n",
    "    g_path = \"cluster_pkls/g{}.pkl\".format(i)\n",
    "    h_path = \"cluster_pkls/h{}.pkl\".format(i)\n",
    "    \n",
    "    with open(g_path, 'wb') as file:\n",
    "        pkl.dump(g, file)\n",
    "\n",
    "    # save hypothesis function to h.pkl\n",
    "    with open(h_path, 'wb') as file:\n",
    "        pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " starting n=5\n",
      "\n",
      "improvement: -2868.878396595457\n",
      "improvement: -2799.426970716344\n",
      "improvement: -3325.141381249421\n",
      "improvement: -2275.5687387023972\n",
      "improvement: -5709.369369322296\n",
      "\n",
      " starting n=10\n",
      "\n",
      "improvement: -2868.878396595457\n",
      "improvement: -2799.426970716344\n",
      "improvement: -3325.141381249421\n",
      "improvement: -2275.5687387023972\n",
      "improvement: -5709.369369322296\n",
      "\n",
      " starting n=20\n",
      "\n",
      "improvement: -2868.878396595457\n",
      "improvement: -2799.426970716344\n",
      "improvement: -3325.141381249421\n",
      "improvement: -2275.5687387023972\n",
      "improvement: -5709.369369322296\n"
     ]
    }
   ],
   "source": [
    "n_list = [5, 10, 20]\n",
    "\n",
    "for n in n_list:\n",
    "    print(\"\\n starting n=\" + str(n) + \"\\n\")\n",
    "    cluster_clf = sk.cluster.KMeans(n_clusters= 5, random_state = 42, n_init=10)\n",
    "    X_scaled = scaler.transform(x_train_subset)\n",
    "    cluster_clf.fit(X_scaled)\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        g = cluster_n(cluster_clf, i)\n",
    "        if (g(x_train_subset).any()):\n",
    "            h = train_basic_h(g)\n",
    "            if (check_local_improvement(g,h) > 0):\n",
    "                print(n, i, check_global_improvement(g,h), g(x_train).sum())\n",
    "                save_cluster_pkls(g,h,i)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Random Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_state(state = 42):\n",
    "    np.random.seed(state)\n",
    "set_random_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset random state to make deterministic\n",
    "def get_random_g(state=42):\n",
    "  set_random_state(state)\n",
    "  weight = .3\n",
    "  random_binary_labels = np.random.rand(len(x_train)) < weight\n",
    "  clf = sk.tree.DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "  clf.fit(x_train, random_binary_labels)\n",
    "  g = clf.predict\n",
    "  g(x_train)\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvements = []\n",
    "for i in range(1000):\n",
    "    g = get_random_g(i)\n",
    "    h = train_basic_h(g)\n",
    "    imp = check_global_improvement(g,h)\n",
    "    if(imp>0):\n",
    "        print(i)\n",
    "        improvements.append((i, imp))\n",
    "        # save_pkls(g,h)\n",
    "\n",
    "\n",
    "sorted_list = sorted(improvements, key=lambda x: x[1])\n",
    "sorted_list.reverse()\n",
    "print(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_random_g(985)\n",
    "h = train_basic_h(g)\n",
    "imp = check_global_improvement(g,h)\n",
    "save_pkls(g,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_model:\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self.clf\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = gb_scaler.transform(X)\n",
    "        return self.clf.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up base\n",
    "gb_scaler = preprocessing.StandardScaler().fit(x_train_subset.values)\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 10, random_state = 42)\n",
    "X_scaled = gb_scaler.transform(x_train_subset.values)\n",
    "clf.fit(X_scaled, y_train_subset.ravel())\n",
    "initial_model_gb = base_model(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1135.4598775148988\n",
      "-1135.4598775148988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "g = manual_groups[42]\n",
    "indices = g(x_train_subset)\n",
    "clf = sk.ensemble.GradientBoostingRegressor(max_depth = 3, n_estimators = 100, random_state = 42, init = initial_model_gb)\n",
    "# calibrate clf with 100 bins on group indices\n",
    "clf.fit(x_train_subset[indices].values, y_train_subset[indices].ravel())\n",
    "# define h for architecture\n",
    "h = clf.predict\n",
    "print(check_global_improvement(g,h))\n",
    "save_pkls(g,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2584.8815566039793\n",
      "0 -2584.8815566039793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1864.2916714188432\n",
      "1 -1864.2916714188432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1965.6078050531924\n",
      "2 -1965.6078050531924\n",
      "improvement: -9121.768439490386\n",
      "3 -9121.768439490386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2707.5784968246444\n",
      "4 -2707.5784968246444\n",
      "improvement: -1420.552399001106\n",
      "6 -1420.552399001106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2106.3948386092234\n",
      "7 -2106.3948386092234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1821.498809636596\n",
      "8 -1821.498809636596\n",
      "improvement: -2384.2385473677423\n",
      "9 -2384.2385473677423\n",
      "improvement: -3926.660846710138\n",
      "11 -3926.660846710138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1232.6361633602755\n",
      "12 -1232.6361633602755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -3959.0756511632844\n",
      "13 -3959.0756511632844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2482.674085246248\n",
      "14 -2482.674085246248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2782.8572231278886\n",
      "15 -2782.8572231278886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -3869.504150972545\n",
      "16 -3869.504150972545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2049.28097423301\n",
      "17 -2049.28097423301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2019.122685540975\n",
      "18 -2019.122685540975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2736.270276300893\n",
      "19 -2736.270276300893\n",
      "improvement: -6959.1522658236245\n",
      "20 -6959.1522658236245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -3774.2261759097837\n",
      "21 -3774.2261759097837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2494.6905739390513\n",
      "22 -2494.6905739390513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1780.4166989207206\n",
      "23 -1780.4166989207206\n",
      "improvement: -1844.8368471089088\n",
      "24 -1844.8368471089088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1540.333085955319\n",
      "25 -1540.333085955319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2962.6289502982254\n",
      "26 -2962.6289502982254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -3250.606207669007\n",
      "27 -3250.606207669007\n",
      "improvement: -1836.923779443683\n",
      "28 -1836.923779443683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1955.9743248264149\n",
      "29 -1955.9743248264149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1990.1430994555067\n",
      "30 -1990.1430994555067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2187.4316921951868\n",
      "31 -2187.4316921951868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2641.5757567261862\n",
      "32 -2641.5757567261862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2786.679704497812\n",
      "33 -2786.679704497812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2242.150386024361\n",
      "34 -2242.150386024361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1843.4472899896118\n",
      "35 -1843.4472899896118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1986.7041731980353\n",
      "36 -1986.7041731980353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2778.454749966859\n",
      "37 -2778.454749966859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2162.9276954065954\n",
      "38 -2162.9276954065954\n",
      "improvement: -4136.883242011707\n",
      "39 -4136.883242011707\n",
      "improvement: -3932.561379022274\n",
      "40 -3932.561379022274\n",
      "improvement: -7736.137942332742\n",
      "41 -7736.137942332742\n",
      "improvement: -1135.4598775148988\n",
      "42 -1135.4598775148988\n",
      "improvement: -8579.235699066445\n",
      "43 -8579.235699066445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2351.914880888213\n",
      "44 -2351.914880888213\n",
      "improvement: -1066.959065313673\n",
      "45 -1066.959065313673\n",
      "improvement: -6584.662570539855\n",
      "46 -6584.662570539855\n",
      "improvement: -1592.57896082761\n",
      "47 -1592.57896082761\n",
      "improvement: -7955.899818926937\n",
      "48 -7955.899818926937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2133.628168918449\n",
      "50 -2133.628168918449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2296.3026011718866\n",
      "51 -2296.3026011718866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -4330.816025445842\n",
      "52 -4330.816025445842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2322.8115610724344\n",
      "53 -2322.8115610724344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2479.295972577849\n",
      "54 -2479.295972577849\n",
      "improvement: -5954.4182498137525\n",
      "55 -5954.4182498137525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2808.4078197543295\n",
      "56 -2808.4078197543295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1708.2891493259212\n",
      "57 -1708.2891493259212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1852.6019518411376\n",
      "58 -1852.6019518411376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -5454.544451262911\n",
      "59 -5454.544451262911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2808.4078197543295\n",
      "60 -2808.4078197543295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1708.2891493259212\n",
      "61 -1708.2891493259212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1852.6019518411376\n",
      "62 -1852.6019518411376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -5454.544451262911\n",
      "63 -5454.544451262911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2786.679704497812\n",
      "64 -2786.679704497812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1956.7684220470292\n",
      "65 -1956.7684220470292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1461.4445304184374\n",
      "66 -1461.4445304184374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1926.7379831676517\n",
      "67 -1926.7379831676517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2505.096776663686\n",
      "68 -2505.096776663686\n",
      "improvement: -2565.8915784139826\n",
      "69 -2565.8915784139826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1714.7057020365755\n",
      "70 -1714.7057020365755\n",
      "improvement: -1889.372955660172\n",
      "71 -1889.372955660172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1843.4472899896118\n",
      "72 -1843.4472899896118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1000.2004209111492\n",
      "73 -1000.2004209111492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2472.766221662936\n",
      "74 -2472.766221662936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1977.1812506808692\n",
      "75 -1977.1812506808692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1283.2828098978644\n",
      "76 -1283.2828098978644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1148.913535628617\n",
      "77 -1148.913535628617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1420.0085163955155\n",
      "78 -1420.0085163955155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1367.1325029289455\n",
      "79 -1367.1325029289455\n",
      "improvement: -2700.101715963483\n",
      "80 -2700.101715963483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1871.0084778086402\n",
      "81 -1871.0084778086402\n",
      "improvement: -3003.755914649555\n",
      "82 -3003.755914649555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -2130.4031165488595\n",
      "83 -2130.4031165488595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvement: -1557.3512364149065\n",
      "84 -1557.3512364149065\n",
      "improvement: -2054.9387637532564\n",
      "85 -2054.9387637532564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define some 'good' clf.\n",
    "for i in range(len(manual_groups)):\n",
    "    g = manual_groups[i]\n",
    "    indices = g(x_train_subset)\n",
    "    # gradient boost base model specific to g\n",
    "    if(indices.any()):\n",
    "        clf = sk.ensemble.GradientBoostingRegressor(max_depth = 3, n_estimators = 100, random_state = 42, init = initial_model_gb)\n",
    "        # calibrate clf with 100 bins on group indices\n",
    "        clf.fit(x_train_subset[indices].values, y_train_subset[indices].ravel())\n",
    "        # define h for architecture\n",
    "        h = clf.predict\n",
    "        print(i, check_global_improvement(g,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
