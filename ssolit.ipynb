{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and make subsets \n",
    "x_train = pd.read_csv('training_data.csv') \n",
    "y_train = np.genfromtxt('training_labels.csv', delimiter=',', dtype = float).reshape(340134,1)\n",
    "\n",
    "x_train_subset, x_val, y_train_subset, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size = .15, random_state = 42)\n",
    "# PDL = PointerDecisionList(base_clf, x_train_subset, y_train_subset, x_val, y_val, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import current info\n",
    "# Note, may not be up to date with global git\n",
    "global_preds_path ='models/global_model/training_predictions.csv'\n",
    "global_preds = pd.read_csv(global_preds_path, header=None).transpose()\n",
    "PAS_preds_path = 'models/PAS/training_predictions.csv'\n",
    "PAS_preds = pd.read_csv(PAS_preds_path, header=None).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(global_preds.shape)\n",
    "print(PAS_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_improvement(old_pred, g, h):\n",
    "    indices = g(x_train)\n",
    "    old_pred = old_pred[indices]\n",
    "    new_pred = h(x_train[indices])\n",
    "    old_RMSE = math.sqrt(mean_squared_error(y_train[indices], old_pred))\n",
    "    new_RMSE = math.sqrt(mean_squared_error(y_train[indices], new_pred))\n",
    "    # print(f\"improvement: {old_RMSE-new_RMSE}\")\n",
    "    # if (old_RMSE-new_RMSE>0):\n",
    "        # print(\"\\n IMPROVEMENT \\n IMPROVEMENT \\n IMPROVEMENT \\n\")\n",
    "    return old_RMSE-new_RMSE\n",
    "\n",
    "def check_local_improvement(g,h):\n",
    "    return check_improvement(PAS_preds, g, h)\n",
    "\n",
    "def check_global_improvement(g,h):\n",
    "    return check_improvement(global_preds, g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_basic_h(g):\n",
    "    clf = sk.tree.DecisionTreeRegressor(max_depth = 7, random_state = 42)\n",
    "\n",
    "    # find group indices on data\n",
    "    indices = g(x_train)\n",
    "\n",
    "    # fit model specifically to group\n",
    "    clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "    # define hypothesis function as bound clf.predict\n",
    "    h = clf.predict\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help minimize start up difficulties, we have provided you with a basic ML workflow for this project, as well as a few possible avenues to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 1: ML Workflow for Submitting *(g,h)* pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Pip Installs and Imports\n",
    "\n",
    "We will be using a package *dill* which is a variant of *pickle*, but allows a bit more expressive byte code serialization. This package is essential to saving your *(g,h)* pairs!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a non-inclusive list of packages you may find helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download/Load Data\n",
    "\n",
    "Navigate to the project [webpage](https://declancharrison.github.io/CIS_5230_Bias_Bounty_2023/) and click \"Download Training Data\". Extract the .zip files in the folder where this notebook is located, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('training_data.csv') \n",
    "y_train = np.genfromtxt('training_labels.csv', delimiter=',', dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a (g,h) pair\n",
    "\n",
    "Below is an example of training a Decision Tree Regressor on individuals identified as white from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 1\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 5, random_state = 42)\n",
    "\n",
    "# find group indices on data\n",
    "indices = g(x_train)\n",
    "\n",
    "# fit model specifically to group\n",
    "clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Save Objects\n",
    "\n",
    "The following cell will save your group model *g* with filename *g.pkl*, and your hypothesis function *h* with filename *h.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group function to g.pkl\n",
    "with open('cluster_pkls/g.pkl', 'wb') as file:\n",
    "    pkl.dump(g, file)\n",
    "\n",
    "# save hypothesis function to h.pkl\n",
    "with open('cluster_pkls/h.pkl', 'wb') as file:\n",
    "    pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Upload Models to Google Drive and Submit PR Request with Links\n",
    "\n",
    "Follow instructions on GitHub Repo to submit a *(g,h)* pair update request!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: Reducing Workflow Time Requirements by Creating a Local PDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probably noticed, submitting a *(g,h)* pair to the GitHub repository can take a long time depending on the current workload of the server. To approximate whether or not an update will be accepted, we have provided you the PDL architecture file and a workflow that will mimic your team's private PDL maintained by the server. \n",
    "\n",
    "**NOTE: One major caveat is the validation data this workflow uses is a cut from the training data, meaning you will want to refrain from training on it to prevent overfitting.**\n",
    "\n",
    "The way we suggest getting around this without losing data efficacy is to train a *(g,h)* pair on the subset of training data that does not include the validation set, and attempt the *(g,h)* pair update on the local PDL. If the pair is rejected, you can continue tuning hyperparameters or searching for new groups. If the pair is accepted, you can retrain a new *(g,h)* pair over ALL the training data, and submit this pair to the server for an update. This will allow you to \"squeeze all the juice\" from your training data and test potential updates much quicker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT CHANGE THIS CELL ###\n",
    "from pdl import PointerDecisionList\n",
    "\n",
    "x_train_subset, x_val, y_train_subset, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size = .15, random_state = 42)\n",
    "base_clf = sk.tree.DecisionTreeRegressor(max_depth = 1, random_state = 42)\n",
    "base_clf.fit(x_train_subset, y_train_subset)\n",
    "PDL = PointerDecisionList(base_clf, x_train_subset, y_train_subset, x_val, y_val, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your *(g,h)* pair on the subset of training data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 2\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 5, random_state = 42)\n",
    "\n",
    "# find group indices on data\n",
    "indices = g(x_train_subset)\n",
    "\n",
    "# fit model specifically to group\n",
    "clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt an update using the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_flag = PDL.update(g, h, x_train_subset, y_train_subset, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put these two together to train a classifier using the whole training dataset after if it has been accepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group function\n",
    "def g(X):\n",
    "    return X['RAC1P'] == 1\n",
    "\n",
    "# initialize ML hypothesis class\n",
    "clf = sk.tree.DecisionTreeRegressor(max_depth = 10, random_state = 42)\n",
    "\n",
    "# find group indices on training subset\n",
    "indices = g(x_train_subset)\n",
    "\n",
    "# fit model specifically to group subset\n",
    "clf.fit(x_train_subset[indices], y_train_subset[indices])\n",
    "\n",
    "# define hypothesis function as bound clf.predict\n",
    "h = clf.predict\n",
    "\n",
    "# compute PDL update\n",
    "update_flag = PDL.update(g, h, x_train_subset, y_train_subset, x_val, y_val)\n",
    "\n",
    "if update_flag:\n",
    "\n",
    "    # recompute indices over whole training dataset\n",
    "    indices = g(x_train)\n",
    "\n",
    "    # refit classifier to full group\n",
    "    clf.fit(x_train[indices], y_train[indices])\n",
    "\n",
    "    # define hypothesis function as bound clf.predict\n",
    "    h = clf.predict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit *(g,h)* pair to GitHub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: You can save your PDL but it will require that your validation set does not change! Thus, you should not change the random state used to split your training data once you create your PDL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save PDL\n",
    "PDL.save_model()\n",
    "\n",
    "# open PDL structure\n",
    "with open('PDL/model.pkl', 'rb') as file:\n",
    "    PDL = pkl.load(file)\n",
    "\n",
    "# reload group/hypothesis functions to PDL\n",
    "PDL.reload_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automated Group Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epsilon above/below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view how different global is from labels\n",
    "abs_diff = (global_preds - y_train).abs()\n",
    "abs_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train clf to identify rows with big difference\n",
    "\n",
    "def epsilon_above(epsilon):\n",
    "    # define 0,1 labels where current predictions OVERESTIMATE by at least epsilon\n",
    "    binary_labels = (global_preds - y_train) < epsilon\n",
    "\n",
    "    # define group classifier class\n",
    "    clf = sk.tree.DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "\n",
    "    # fit classifier to binary labels\n",
    "    clf.fit(x_train, binary_labels)\n",
    "\n",
    "    # define g\n",
    "    g = clf.predict\n",
    "    # visualize results\n",
    "    # pd.DataFrame(g(x_train).astype(int)).describe()\n",
    "    \n",
    "    return g\n",
    "\n",
    "def epsilon_below(epsilon):\n",
    "    # define 0,1 labels where current predictions OVERESTIMATE by at least epsilon\n",
    "    binary_labels = (y_train - global_preds) < epsilon\n",
    "\n",
    "    # define group classifier class\n",
    "    clf = sk.tree.DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "\n",
    "    # fit classifier to binary labels\n",
    "    clf.fit(x_train, binary_labels)\n",
    "\n",
    "    # define g\n",
    "    g = clf.predict\n",
    "    # visualize results\n",
    "    # pd.DataFrame(g(x_train).astype(int)).describe()\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    g = epsilon_below(i*1000)\n",
    "    h = train_basic_h(g)\n",
    "    print(str(i*1000))\n",
    "    check_global_improvement(g,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class targeted_correction:\n",
    "    def __init__(self, clf, value, epsilon):\n",
    "        self.clf = clf\n",
    "        self.value = value\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = self.clf.predict(X)\n",
    "        return abs(predictions - self.value) < self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "for i in range(100):\n",
    "    g = targeted_correction(clf, i*1000, 5000)\n",
    "    indices = g(x_train)\n",
    "    old_RMSE = math.sqrt(mean_squared_error(y_train[indices], global_preds[indices]))\n",
    "    print(old_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100):\n",
    "    # print(i*1000)\n",
    "    for j in range(5, 35):\n",
    "        v = i*1000\n",
    "        e = j*400\n",
    "        g = targeted_correction(clf, e, j)\n",
    "        if (not (g(x_train)==0).all()): # check if 0 for all predictions to avoid error\n",
    "            print(str(v), str(e))\n",
    "            h = train_basic_h(g)\n",
    "            if (check_global_improvement(g,h) > 0):\n",
    "                print(v,e)\n",
    "                break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False,  True])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cluster_n:\n",
    "\n",
    "    def __init__(self, clf, n):\n",
    "        # define attibutes here. You may add more parameters to the init method (see example below)\n",
    "        self.clf = clf  \n",
    "        self.n = n\n",
    "\n",
    "    # DO NOT CHANGE CALL FUNCTION, FORMAT .predict\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # find instances where cluster is 1\n",
    "        return self.clf.predict(X) == self.n\n",
    "\n",
    "    \n",
    "cluster_clf = sk.cluster.KMeans(n_clusters= 5, random_state = 42)\n",
    "cluster_clf.fit(x_train)\n",
    "g = cluster_n(cluster_clf, 1)\n",
    "\n",
    "# visualize results\n",
    "g(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " starting n=100\n",
      "\n",
      "100 8 1210.9636548417839 903\n",
      "100 11 651.6711999040526 1920\n",
      "100 12 355.0483177453916 1110\n",
      "100 21 244.57656546830913 2460\n",
      "100 23 1196.7789898890805 1030\n",
      "100 27 1129.2207599268186 763\n",
      "100 32 418.1430378817604 1295\n",
      "100 35 82.1676859444051 3825\n",
      "100 44 176.93248377819145 2317\n",
      "100 45 1114.810844262258 959\n",
      "100 49 2846.9621337024873 340\n",
      "100 51 15.101148062376524 3767\n",
      "100 52 265.1659412173358 954\n",
      "100 53 244.31999900599294 1964\n",
      "100 61 36.70977274042525 2754\n",
      "100 62 163.86897497259451 3426\n",
      "100 67 338.0181905722693 857\n",
      "100 70 926.831371248596 828\n",
      "100 78 390.9525104534441 1303\n",
      "100 79 959.1681221623403 1006\n",
      "100 80 6.62241010802245 1280\n",
      "100 82 886.087105286424 892\n",
      "100 84 1246.7705163471073 550\n",
      "100 86 192.10248022272572 1691\n",
      "100 88 85.96495988819697 4099\n",
      "100 89 3.5662618013120664 8196\n",
      "100 90 432.775939060597 1269\n",
      "100 92 168.3088534522467 1844\n",
      "100 93 188.40742932956164 3165\n",
      "100 94 1470.4657391122491 776\n",
      "100 95 393.2179664145042 1764\n",
      "100 98 43.77250848244694 3447\n"
     ]
    }
   ],
   "source": [
    "n_list = [100]\n",
    "\n",
    "for n in n_list:\n",
    "    print(\"\\n starting n=\" + str(n) + \"\\n\")\n",
    "    cluster_clf = sk.cluster.KMeans(n_clusters=n, random_state = 42)\n",
    "    cluster_clf.fit(x_train)\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        g = cluster_n(cluster_clf, i)\n",
    "        h = train_basic_h(g)\n",
    "        if (check_global_improvement(g,h) > 0):\n",
    "            print(n, i, check_global_improvement(g,h), g(x_train).sum())\n",
    "            save_cluster_pkls(g,h,i)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_pkls(g, h, i):\n",
    "    # save group function to g.pkl\n",
    "    g_path = \"cluster_pkls/g{}.pkl\".format(i)\n",
    "    h_path = \"cluster_pkls/h{}.pkl\".format(i)\n",
    "    \n",
    "    with open(g_path, 'wb') as file:\n",
    "        pkl.dump(g, file)\n",
    "\n",
    "    # save hypothesis function to h.pkl\n",
    "    with open(h_path, 'wb') as file:\n",
    "        pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
